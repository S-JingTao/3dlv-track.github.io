<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Language-to-4D Modeling Towards\\ 6-DoF Tracking and Shape Reconstruction in 3D Point Cloud Stream.">
  <meta name="keywords" content="tracking, NeRF, pose estimation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>L4D-Track: Language-to-4D Modeling Towards 6-DoF Tracking and Shape Reconstruction in 3D Point Cloud Stream</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/github_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">L4D-Track: Language-to-4D Modeling Towards 6-DoF Tracking and Shape Reconstruction in 3D Point Cloud Stream</h1>
          <div class="is-size-5 publication-authors">
<!--             <span class="author-block">
              Jingtao Sun</a><sup>1,2</sup>,</span>
            <span class="author-block">
              Yaonan Wang</a><sup>2</sup>,</span>
            <span class="author-block">
              Mingtao Feng</a><sup>2</sup>,
            </span> -->
<!--             <span class="author-block">
              Dongxing Mao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Weixian Lei</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Jiawei Liu</a><sup>1</sup>,
            </span> -->
<!--             <span class="author-block">
              Yulan Guo</a><sup>3</sup>
            </span>
            <span class="author-block">
              Ajmal Mian</a><sup>4</sup>
            </span>
            <span class="author-block">
              Mike Zheng Shou</a><sup>1</sup>
            </span> -->
          </div>

<!--           <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National University of Singapore,</span>
            <span class="author-block"><sup>2</sup>Hunan University</span>
            <span class="author-block"><sup>3</sup>Sun Yat-Sen University,</span>
            <span class="author-block"><sup>4</sup>The University of Western Australia</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=ZUV1spTJns&referrer=%5Bthe%20profile%20of%20Jingtao%20Sun%5D(%2Fprofile%3Fid%3D~Jingtao_Sun2)"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://anonymous.4open.science/r/L4D_Track-code-6648/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          3D visual language multi-modal modeling plays an important role in actual human-computer interaction. 
          However, the inaccessibility of large-scale 3D-language pairs restricts their applicability in real-world scenarios. 
          In this paper, we aim to handle a real-time multi-task for 6-DoF pose tracking of unknown objects, leveraging 3D-language 
          pre-training scheme from a series of 3D point cloud video streams, while simultaneously performing 3D shape reconstruction 
          in current observation. To this end, we present a generic \underline{L}anguage-to-\underline{4D} modeling paradigm termed L4D-Track, 
          that tackles zero-shot 6-DoF \underline{Track}ing and shape reconstructing by learning pairwise implicit 3D representation and 
          multi-modal features alignment. Our method constitutes two core parts. 1) Pairwise Implicit 3D Space Representation, that establishes 
          spatial-temporal to language coherence descriptions across continuous 3D point cloud video. 2) 3D Video-Language Association and Contrastive 
          Alignment, enables multi-modality semantic connections between 3D point cloud video and language. Our method trained exclusively on public 
          NOCS-REAL275 dataset, achieves promising results on both two publicly benchmarks. This not only shows powerful generalization performance, 
          but also proves its remarkable capability in zero-shot inference.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- pipline. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pipline</h2>
        <div class="publication-video">
          <img src="./figure/show.png"></img>
          Given a 3D point cloud stream and the language-3D captions, our method achieves real-time, causal 6-DoF pose tracking while
          reconstructing the 3D shape in the current observation. We demonstrate that: (a) our method not only enables zero-shot inference for
          unseen objects with known categories, (b) but also perfectly showcases the zero-shot capabilities for unseen objects with unknown classes.
          </p>
        </div>
      </div>
    </div>
    <!--/ pipline. -->
    <!-- overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <img src="./figure/overview.png" alt="Image description" style="width: 100%; height: 100%;"></img>
          
          Illustration of the pipline of proposed methodology. Given input point cloud stream along with the corresponding segmented
          mask, we first encode them with both 2D/3D backbone and cross-coupled fusion module separately to obtain inter-frame embeddings
          ft−1, ft. These paired embeddings are then used to model the energy-based hypothesis about changes in pose and learn a neural pose-
          aligned field that generates shape query while aligning its pose for an arbitrary object. Meanwhile, these embeddings will be aligned
          with the extra input multi-level language instructions using proposed GPT-assisted assocaition and alignment modules to achieve zero-shot
          inference. It’s noteworthy that the caption embeddings fc are added into ft−1, ft to enhance its performance during the inference stage.
        </div>
      </div>
    </div>
    <!--/ overview. -->
    <!-- visualization. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visualization Results</h2>
        <div class="content has-text-centered">
          <img src="./figure/visualization.png" alt="Image description" style="width: 100%; height: 100%;"></img>
          <img src="./figure/extra-show-real.png" alt="Image description" style="width: 100%; height: 100%;"></img>
        </div>
      </div>
    </div>
    <!--/ visualization. -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{sun2023l4d,
  author    = {Jingtao Sun, Yaonan Wang, Mingtao Feng, Yulan Guo, Ajmal Mian and Mike Zheng Shou},
  title     = {L4D-Track: Language-to-4D Modeling Towards 6-DoF Tracking and Shape Reconstruction in 3D Point Cloud Stream},
  journal   = {},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/S-JingTao" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
